{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "kXqg-4alzi_H"
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n",
        "# https://pytorch.org/tutorials/beginner/colab\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-0FuaNKt7en",
        "outputId": "4ca56ef9-310e-4a43-da2c-18ee08a599ba"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code sets up the necessary libraries/modules and checks whether CUDA (GPU support) is available, assigning the appropriate device (\"cuda\" or \"cpu\") for computation:"
      ],
      "metadata": {
        "id": "rT4TMRKkvq54"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "K8_1ROHTzi_P"
      },
      "outputs": [],
      "source": [
        "# Import the necessary libraries/modules\n",
        "import torch  # PyTorch library for deep learning\n",
        "from torch.jit import script, trace  # For script and trace in PyTorch\n",
        "import torch.nn as nn  # Neural network module in PyTorch\n",
        "from torch import optim  # Optimisation module in PyTorch\n",
        "import torch.nn.functional as F  # Functional operations for neural networks\n",
        "import csv  # Comma Separated Values (CSV) file handling\n",
        "import random  # Random number generation\n",
        "import re  # Regular expressions for text processing\n",
        "import os  # Operating system interface\n",
        "import unicodedata  # Unicode character database\n",
        "import codecs  # Codec support for reading and writing text files\n",
        "from io import open  # Input/output operations with text files\n",
        "import itertools  # Iteration utilities\n",
        "import math  # Mathematical operations\n",
        "import json  # JSON (JavaScript Object Notation) handling\n",
        "\n",
        "# Check if CUDA (GPU support) is available, and set the device accordingly\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code defines a function printLines that reads and prints the first 'n' lines from a given file. Then, it calls this function with the path to the \"utterances.jsonl\" file within the \"movie-corpus\" directory:"
      ],
      "metadata": {
        "id": "z8Drcefmv_vk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "QCJtfxyhzi_Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "baf1707c-9341-461f-be94-2c8e0d709066"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'{\"id\": \"L1045\", \"conversation_id\": \"L1044\", \"text\": \"They do not!\", \"speaker\": \"u0\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 1, \"toks\": [{\"tok\": \"They\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 1, \"dn\": []}, {\"tok\": \"do\", \"tag\": \"VBP\", \"dep\": \"ROOT\", \"dn\": [0, 2, 3]}, {\"tok\": \"not\", \"tag\": \"RB\", \"dep\": \"neg\", \"up\": 1, \"dn\": []}, {\"tok\": \"!\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 1, \"dn\": []}]}]}, \"reply-to\": \"L1044\", \"timestamp\": null, \"vectors\": []}\\n'\n",
            "b'{\"id\": \"L1044\", \"conversation_id\": \"L1044\", \"text\": \"They do to!\", \"speaker\": \"u2\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 1, \"toks\": [{\"tok\": \"They\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 1, \"dn\": []}, {\"tok\": \"do\", \"tag\": \"VBP\", \"dep\": \"ROOT\", \"dn\": [0, 2, 3]}, {\"tok\": \"to\", \"tag\": \"TO\", \"dep\": \"dobj\", \"up\": 1, \"dn\": []}, {\"tok\": \"!\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 1, \"dn\": []}]}]}, \"reply-to\": null, \"timestamp\": null, \"vectors\": []}\\n'\n",
            "b'{\"id\": \"L985\", \"conversation_id\": \"L984\", \"text\": \"I hope so.\", \"speaker\": \"u0\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 1, \"toks\": [{\"tok\": \"I\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 1, \"dn\": []}, {\"tok\": \"hope\", \"tag\": \"VBP\", \"dep\": \"ROOT\", \"dn\": [0, 2, 3]}, {\"tok\": \"so\", \"tag\": \"RB\", \"dep\": \"advmod\", \"up\": 1, \"dn\": []}, {\"tok\": \".\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 1, \"dn\": []}]}]}, \"reply-to\": \"L984\", \"timestamp\": null, \"vectors\": []}\\n'\n",
            "b'{\"id\": \"L984\", \"conversation_id\": \"L984\", \"text\": \"She okay?\", \"speaker\": \"u2\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 1, \"toks\": [{\"tok\": \"She\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 1, \"dn\": []}, {\"tok\": \"okay\", \"tag\": \"RB\", \"dep\": \"ROOT\", \"dn\": [0, 2]}, {\"tok\": \"?\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 1, \"dn\": []}]}]}, \"reply-to\": null, \"timestamp\": null, \"vectors\": []}\\n'\n",
            "b'{\"id\": \"L925\", \"conversation_id\": \"L924\", \"text\": \"Let\\'s go.\", \"speaker\": \"u0\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 0, \"toks\": [{\"tok\": \"Let\", \"tag\": \"VB\", \"dep\": \"ROOT\", \"dn\": [2, 3]}, {\"tok\": \"\\'s\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 2, \"dn\": []}, {\"tok\": \"go\", \"tag\": \"VB\", \"dep\": \"ccomp\", \"up\": 0, \"dn\": [1]}, {\"tok\": \".\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 0, \"dn\": []}]}]}, \"reply-to\": \"L924\", \"timestamp\": null, \"vectors\": []}\\n'\n",
            "b'{\"id\": \"L924\", \"conversation_id\": \"L924\", \"text\": \"Wow\", \"speaker\": \"u2\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 0, \"toks\": [{\"tok\": \"Wow\", \"tag\": \"UH\", \"dep\": \"ROOT\", \"dn\": []}]}]}, \"reply-to\": null, \"timestamp\": null, \"vectors\": []}\\n'\n",
            "b'{\"id\": \"L872\", \"conversation_id\": \"L870\", \"text\": \"Okay -- you\\'re gonna need to learn how to lie.\", \"speaker\": \"u0\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 4, \"toks\": [{\"tok\": \"Okay\", \"tag\": \"UH\", \"dep\": \"intj\", \"up\": 4, \"dn\": []}, {\"tok\": \"--\", \"tag\": \":\", \"dep\": \"punct\", \"up\": 4, \"dn\": []}, {\"tok\": \"you\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 4, \"dn\": []}, {\"tok\": \"\\'re\", \"tag\": \"VBP\", \"dep\": \"aux\", \"up\": 4, \"dn\": []}, {\"tok\": \"gon\", \"tag\": \"VBG\", \"dep\": \"ROOT\", \"dn\": [0, 1, 2, 3, 6, 12]}, {\"tok\": \"na\", \"tag\": \"TO\", \"dep\": \"aux\", \"up\": 6, \"dn\": []}, {\"tok\": \"need\", \"tag\": \"VB\", \"dep\": \"xcomp\", \"up\": 4, \"dn\": [5, 8]}, {\"tok\": \"to\", \"tag\": \"TO\", \"dep\": \"aux\", \"up\": 8, \"dn\": []}, {\"tok\": \"learn\", \"tag\": \"VB\", \"dep\": \"xcomp\", \"up\": 6, \"dn\": [7, 11]}, {\"tok\": \"how\", \"tag\": \"WRB\", \"dep\": \"advmod\", \"up\": 11, \"dn\": []}, {\"tok\": \"to\", \"tag\": \"TO\", \"dep\": \"aux\", \"up\": 11, \"dn\": []}, {\"tok\": \"lie\", \"tag\": \"VB\", \"dep\": \"xcomp\", \"up\": 8, \"dn\": [9, 10]}, {\"tok\": \".\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 4, \"dn\": []}]}]}, \"reply-to\": \"L871\", \"timestamp\": null, \"vectors\": []}\\n'\n",
            "b'{\"id\": \"L871\", \"conversation_id\": \"L870\", \"text\": \"No\", \"speaker\": \"u2\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 0, \"toks\": [{\"tok\": \"No\", \"tag\": \"UH\", \"dep\": \"ROOT\", \"dn\": []}]}]}, \"reply-to\": \"L870\", \"timestamp\": null, \"vectors\": []}\\n'\n",
            "b'{\"id\": \"L870\", \"conversation_id\": \"L870\", \"text\": \"I\\'m kidding.  You know how sometimes you just become this \\\\\"persona\\\\\"?  And you don\\'t know how to quit?\", \"speaker\": \"u0\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 2, \"toks\": [{\"tok\": \"I\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 2, \"dn\": []}, {\"tok\": \"\\'m\", \"tag\": \"VBP\", \"dep\": \"aux\", \"up\": 2, \"dn\": []}, {\"tok\": \"kidding\", \"tag\": \"VBG\", \"dep\": \"ROOT\", \"dn\": [0, 1, 3]}, {\"tok\": \".\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 2, \"dn\": [4]}, {\"tok\": \" \", \"tag\": \"_SP\", \"dep\": \"\", \"up\": 3, \"dn\": []}]}, {\"rt\": 1, \"toks\": [{\"tok\": \"You\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 1, \"dn\": []}, {\"tok\": \"know\", \"tag\": \"VBP\", \"dep\": \"ROOT\", \"dn\": [0, 6, 11]}, {\"tok\": \"how\", \"tag\": \"WRB\", \"dep\": \"advmod\", \"up\": 3, \"dn\": []}, {\"tok\": \"sometimes\", \"tag\": \"RB\", \"dep\": \"advmod\", \"up\": 6, \"dn\": [2]}, {\"tok\": \"you\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 6, \"dn\": []}, {\"tok\": \"just\", \"tag\": \"RB\", \"dep\": \"advmod\", \"up\": 6, \"dn\": []}, {\"tok\": \"become\", \"tag\": \"VBP\", \"dep\": \"ccomp\", \"up\": 1, \"dn\": [3, 4, 5, 9]}, {\"tok\": \"this\", \"tag\": \"DT\", \"dep\": \"det\", \"up\": 9, \"dn\": []}, {\"tok\": \"\\\\\"\", \"tag\": \"``\", \"dep\": \"punct\", \"up\": 9, \"dn\": []}, {\"tok\": \"persona\", \"tag\": \"NN\", \"dep\": \"attr\", \"up\": 6, \"dn\": [7, 8, 10]}, {\"tok\": \"\\\\\"\", \"tag\": \"\\'\\'\", \"dep\": \"punct\", \"up\": 9, \"dn\": []}, {\"tok\": \"?\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 1, \"dn\": [12]}, {\"tok\": \" \", \"tag\": \"_SP\", \"dep\": \"\", \"up\": 11, \"dn\": []}]}, {\"rt\": 4, \"toks\": [{\"tok\": \"And\", \"tag\": \"CC\", \"dep\": \"cc\", \"up\": 4, \"dn\": []}, {\"tok\": \"you\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 4, \"dn\": []}, {\"tok\": \"do\", \"tag\": \"VBP\", \"dep\": \"aux\", \"up\": 4, \"dn\": []}, {\"tok\": \"n\\'t\", \"tag\": \"RB\", \"dep\": \"neg\", \"up\": 4, \"dn\": []}, {\"tok\": \"know\", \"tag\": \"VB\", \"dep\": \"ROOT\", \"dn\": [0, 1, 2, 3, 7, 8]}, {\"tok\": \"how\", \"tag\": \"WRB\", \"dep\": \"advmod\", \"up\": 7, \"dn\": []}, {\"tok\": \"to\", \"tag\": \"TO\", \"dep\": \"aux\", \"up\": 7, \"dn\": []}, {\"tok\": \"quit\", \"tag\": \"VB\", \"dep\": \"xcomp\", \"up\": 4, \"dn\": [5, 6]}, {\"tok\": \"?\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 4, \"dn\": []}]}]}, \"reply-to\": null, \"timestamp\": null, \"vectors\": []}\\n'\n",
            "b'{\"id\": \"L869\", \"conversation_id\": \"L866\", \"text\": \"Like my fear of wearing pastels?\", \"speaker\": \"u0\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 0, \"toks\": [{\"tok\": \"Like\", \"tag\": \"IN\", \"dep\": \"ROOT\", \"dn\": [2, 6]}, {\"tok\": \"my\", \"tag\": \"PRP$\", \"dep\": \"poss\", \"up\": 2, \"dn\": []}, {\"tok\": \"fear\", \"tag\": \"NN\", \"dep\": \"pobj\", \"up\": 0, \"dn\": [1, 3]}, {\"tok\": \"of\", \"tag\": \"IN\", \"dep\": \"prep\", \"up\": 2, \"dn\": [4]}, {\"tok\": \"wearing\", \"tag\": \"VBG\", \"dep\": \"pcomp\", \"up\": 3, \"dn\": [5]}, {\"tok\": \"pastels\", \"tag\": \"NNS\", \"dep\": \"dobj\", \"up\": 4, \"dn\": []}, {\"tok\": \"?\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 0, \"dn\": []}]}]}, \"reply-to\": \"L868\", \"timestamp\": null, \"vectors\": []}\\n'\n"
          ]
        }
      ],
      "source": [
        "import os  # Import the 'os' module to work with file paths.\n",
        "\n",
        "# Define the name of the corpus folder.\n",
        "corpus_name = \"movie-corpus\"\n",
        "\n",
        "# Create a file path by joining \"data\" and the corpus_name.\n",
        "corpus = os.path.join(\"movie-corpus\", corpus_name)\n",
        "\n",
        "# Define a function called 'printLines' that takes a file path and an optional argument 'n'.\n",
        "def printLines(file, n=10):\n",
        "    # Open the file in binary read mode and assign it to the 'datafile' variable.\n",
        "    with open(file, 'rb') as datafile:\n",
        "        # Read all lines from the file and store them in the 'lines' list.\n",
        "        lines = datafile.readlines()\n",
        "\n",
        "    # Loop through the first 'n' lines (default is 10) in the 'lines' list.\n",
        "    for line in lines[:n]:\n",
        "        # Print each line to the console.\n",
        "        print(line)\n",
        "\n",
        "# Call the 'printLines' function with the file path to \"utterances.jsonl\" in the 'corpus' folder.\n",
        "printLines(os.path.join(corpus, \"utterances.jsonl\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These functions are used for loading lines and conversations from a file, as well as extracting pairs of sentences from those conversations:"
      ],
      "metadata": {
        "id": "pwny8N3fwQ2k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "KyEito6xzi_R"
      },
      "outputs": [],
      "source": [
        "# Function to load lines and conversations from a file\n",
        "def loadLinesAndConversations(fileName):\n",
        "    lines = {}  # Dictionary to store individual lines\n",
        "    conversations = {}  # Dictionary to store conversations\n",
        "    with open(fileName, 'r', encoding='iso-8859-1') as f:\n",
        "        for line in f:\n",
        "            lineJson = json.loads(line)  # Parse the JSON data from each line\n",
        "            # Extract fields for line object\n",
        "            lineObj = {}\n",
        "            lineObj[\"lineID\"] = lineJson[\"id\"]\n",
        "            lineObj[\"characterID\"] = lineJson[\"speaker\"]\n",
        "            lineObj[\"text\"] = lineJson[\"text\"]\n",
        "            lines[lineObj['lineID']] = lineObj  # Store the line object in the 'lines' dictionary\n",
        "\n",
        "            # Extract fields for conversation object\n",
        "            if lineJson[\"conversation_id\"] not in conversations:\n",
        "                convObj = {}\n",
        "                convObj[\"conversationID\"] = lineJson[\"conversation_id\"]\n",
        "                convObj[\"movieID\"] = lineJson[\"meta\"][\"movie_id\"]\n",
        "                convObj[\"lines\"] = [lineObj]\n",
        "            else:\n",
        "                convObj = conversations[lineJson[\"conversation_id\"]]\n",
        "                convObj[\"lines\"].insert(0, lineObj)\n",
        "            conversations[convObj[\"conversationID\"]] = convObj  # Store the conversation object in the 'conversations' dictionary\n",
        "\n",
        "    return lines, conversations\n",
        "\n",
        "\n",
        "# Function to extract pairs of sentences from conversations\n",
        "def extractSentencePairs(conversations):\n",
        "    qa_pairs = []  # List to store question-answer pairs\n",
        "    for conversation in conversations.values():\n",
        "        # Iterate over all the lines of the conversation\n",
        "        for i in range(len(conversation[\"lines\"]) - 1):  # We ignore the last line\n",
        "            inputLine = conversation[\"lines\"][i][\"text\"].strip()\n",
        "            targetLine = conversation[\"lines\"][i+1][\"text\"].strip()\n",
        "            # Filter out wrong samples (if one of the lists is empty)\n",
        "            if inputLine and targetLine:\n",
        "                qa_pairs.append([inputLine, targetLine])  # Add the pair to the list\n",
        "    return qa_pairs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnIkhMULzi_R"
      },
      "source": [
        "This code prepares and processes data from the \"utterances.jsonl\" file, formats it, and saves it to a new CSV file named \"formatted_movie_lines.txt.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "6grhS9BSzi_S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ac82359-5121-4b56-dea1-0b0039f3daea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing corpus into lines and conversations...\n",
            "\n",
            "Writing newly formatted file...\n",
            "\n",
            "Sample lines from file:\n",
            "b'They do to!\\tThey do not!\\n'\n",
            "b'She okay?\\tI hope so.\\n'\n",
            "b\"Wow\\tLet's go.\\n\"\n",
            "b'\"I\\'m kidding.  You know how sometimes you just become this \"\"persona\"\"?  And you don\\'t know how to quit?\"\\tNo\\n'\n",
            "b\"No\\tOkay -- you're gonna need to learn how to lie.\\n\"\n",
            "b\"I figured you'd get to the good stuff eventually.\\tWhat good stuff?\\n\"\n",
            "b'What good stuff?\\t\"The \"\"real you\"\".\"\\n'\n",
            "b'\"The \"\"real you\"\".\"\\tLike my fear of wearing pastels?\\n'\n",
            "b'do you listen to this crap?\\tWhat crap?\\n'\n",
            "b\"What crap?\\tMe.  This endless ...blonde babble. I'm like, boring myself.\\n\"\n"
          ]
        }
      ],
      "source": [
        "# Define the path to the new file where formatted data will be saved\n",
        "datafile = os.path.join(corpus, \"formatted_movie_lines.txt\")\n",
        "\n",
        "# Define the delimiter used to separate values in the CSV file\n",
        "delimiter = '\\t'\n",
        "\n",
        "# Unescape the delimiter to handle escape sequences if necessary\n",
        "delimiter = str(codecs.decode(delimiter, \"unicode_escape\"))\n",
        "\n",
        "# Initialise dictionaries to store lines and conversations\n",
        "lines = {}  # Dictionary to store individual lines\n",
        "conversations = {}  # Dictionary to store conversations\n",
        "\n",
        "# Load lines and conversations from the \"utterances.jsonl\" file\n",
        "print(\"\\nProcessing corpus into lines and conversations...\")\n",
        "lines, conversations = loadLinesAndConversations(os.path.join(corpus, \"utterances.jsonl\"))\n",
        "\n",
        "# Write the newly formatted data to a CSV file\n",
        "print(\"\\nWriting newly formatted file...\")\n",
        "with open(datafile, 'w', encoding='utf-8') as outputfile:\n",
        "    writer = csv.writer(outputfile, delimiter=delimiter, lineterminator='\\n')\n",
        "    for pair in extractSentencePairs(conversations):\n",
        "        writer.writerow(pair)\n",
        "\n",
        "# Print a sample of lines from the newly created file\n",
        "print(\"\\nSample lines from file:\")\n",
        "printLines(datafile)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOEP-YQNzi_S"
      },
      "source": [
        "This code defines a 'Voc' class for managing a vocabulary, adding words and sentences to it, and trimming it by removing words with low counts:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "3MpOOoDZzi_S"
      },
      "outputs": [],
      "source": [
        "# Define default word tokens\n",
        "PAD_token = 0  # Used for padding short sentences\n",
        "SOS_token = 1  # Start-of-sentence token\n",
        "EOS_token = 2  # End-of-sentence token\n",
        "\n",
        "# Define a vocabulary class to manage word-to-index and index-to-word mappings\n",
        "class Voc:\n",
        "    def __init__(self, name):\n",
        "        self.name = name  # Name of the vocabulary\n",
        "        self.trimmed = False  # Flag to indicate if vocabulary has been trimmed\n",
        "        self.word2index = {}  # Dictionary mapping words to their index\n",
        "        self.word2count = {}  # Dictionary to store word counts\n",
        "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}  # Dictionary mapping indices to words with default tokens\n",
        "        self.num_words = 3  # Initialise the count of words with default tokens (PAD, SOS, EOS)\n",
        "\n",
        "    # Method to add a sentence to the vocabulary\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    # Method to add a word to the vocabulary\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            # If the word is not in the vocabulary, add it\n",
        "            self.word2index[word] = self.num_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.num_words] = word\n",
        "            self.num_words += 1\n",
        "        else:\n",
        "            # If the word is already in the vocabulary, increment its count\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "    # Method to trim the vocabulary by removing words below a certain count threshold\n",
        "    def trim(self, min_count):\n",
        "        if self.trimmed:\n",
        "            return\n",
        "        self.trimmed = True\n",
        "\n",
        "        keep_words = []\n",
        "\n",
        "        for k, v in self.word2count.items():\n",
        "            if v >= min_count:\n",
        "                keep_words.append(k)\n",
        "\n",
        "        print('keep_words {} / {} = {:.4f}'.format(\n",
        "            len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)\n",
        "        ))\n",
        "\n",
        "        # Reinitialise dictionaries\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
        "        self.num_words = 3  # Reset word count to include default tokens\n",
        "\n",
        "        for word in keep_words:\n",
        "            self.addWord(word)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcYbL2Xlzi_T"
      },
      "source": [
        "This code prepares training data by reading and normalizing pairs of sentences, filtering them based on sentence length, and creating a vocabulary object ('Voc') to manage word-to-index mappings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "f8FWpwXWzi_T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "591c0763-c129-4e95-e814-032b8bba38c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start preparing training data ...\n",
            "Reading lines...\n",
            "Read 221282 sentence pairs\n",
            "Trimmed to 145493 sentence pairs\n",
            "Counting words...\n",
            "Counted words: 33170\n",
            "\n",
            "pairs:\n",
            "['they do to !', 'they do not !']\n",
            "['she okay ?', 'i hope so .']\n",
            "['wow', 'let s go .']\n",
            "['no', 'okay you re gonna need to learn how to lie .']\n",
            "['i figured you d get to the good stuff eventually .', 'what good stuff ?']\n",
            "['what good stuff ?', 'the real you .']\n",
            "['the real you .', 'like my fear of wearing pastels ?']\n",
            "['do you listen to this crap ?', 'what crap ?']\n",
            "['what crap ?', 'me . this endless . . .blonde babble . i m like boring myself .']\n",
            "['me . this endless . . .blonde babble . i m like boring myself .', 'thank god ! if i had to hear one more story about your coiffure . . .']\n"
          ]
        }
      ],
      "source": [
        "# Maximum sentence length to consider\n",
        "MAX_LENGTH = 20\n",
        "\n",
        "# Function to convert Unicode string to plain ASCII\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Function to normalize a string: lowercase, trim, and remove non-letter characters\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())  # Convert to lowercase and strip spaces\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)  # Add spaces around punctuation\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)  # Remove non-letter characters\n",
        "    s = re.sub(r\"\\s+\", r\" \", s).strip()  # Remove extra spaces and strip\n",
        "    return s\n",
        "\n",
        "# Function to read query/response pairs and return a 'Voc' object and pairs list\n",
        "def readVocs(datafile, corpus_name):\n",
        "    print(\"Reading lines...\")\n",
        "    # Read the file and split it into lines\n",
        "    lines = open(datafile, encoding='utf-8').read().strip().split('\\n')\n",
        "    # Split every line into pairs and normalize them\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "    # Create a 'Voc' object with the given 'corpus_name'\n",
        "    voc = Voc(corpus_name)\n",
        "    return voc, pairs\n",
        "\n",
        "# Function to filter pairs based on sentence length (under MAX_LENGTH)\n",
        "def filterPair(p):\n",
        "    # Input sequences need to preserve the last word for EOS token\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH\n",
        "\n",
        "# Function to filter pairs using the 'filterPair' condition\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]\n",
        "\n",
        "# Function to load and prepare training data, returning a populated 'Voc' object and pairs list\n",
        "def loadPrepareData(corpus, corpus_name, datafile, save_dir):\n",
        "    print(\"Start preparing training data ...\")\n",
        "    voc, pairs = readVocs(datafile, corpus_name)\n",
        "    print(\"Read {!s} sentence pairs\".format(len(pairs)))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to {!s} sentence pairs\".format(len(pairs)))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        voc.addSentence(pair[0])\n",
        "        voc.addSentence(pair[1])\n",
        "    print(\"Counted words:\", voc.num_words)\n",
        "    return voc, pairs\n",
        "\n",
        "# Define a save directory for the 'Voc' object and pairs list\n",
        "save_dir = os.path.join(\"data\", \"save\")\n",
        "\n",
        "# Load and prepare data using the functions defined above\n",
        "voc, pairs = loadPrepareData(corpus, corpus_name, datafile, save_dir)\n",
        "\n",
        "# Print some example pairs to validate the data\n",
        "print(\"\\npairs:\")\n",
        "for pair in pairs[:10]:\n",
        "    print(pair)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpdDsCQxzi_T"
      },
      "source": [
        "This code defines a function 'trimRareWords' to trim rare words from the vocabulary ('voc') and filter pairs that do not contain trimmed words in their input or output sentences:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Pu5VIRagzi_T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46c63a33-6670-4277-f789-31a1cd3fa108"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "keep_words 17175 / 33167 = 0.5178\n",
            "Trimmed from 145493 pairs to 126905, 0.8722 of total\n"
          ]
        }
      ],
      "source": [
        "# Minimum word count threshold for trimming\n",
        "MIN_COUNT = 3\n",
        "\n",
        "# Function to trim rare words from the vocabulary and filter pairs\n",
        "def trimRareWords(voc, pairs, MIN_COUNT):\n",
        "    # Trim words used less than MIN_COUNT times from the vocabulary\n",
        "    voc.trim(MIN_COUNT)\n",
        "\n",
        "    # Initialise a list to keep pairs that do not contain trimmed words\n",
        "    keep_pairs = []\n",
        "\n",
        "    for pair in pairs:\n",
        "        input_sentence = pair[0]\n",
        "        output_sentence = pair[1]\n",
        "        keep_input = True\n",
        "        keep_output = True\n",
        "\n",
        "        # Check input sentence\n",
        "        for word in input_sentence.split(' '):\n",
        "            if word not in voc.word2index:\n",
        "                keep_input = False\n",
        "                break\n",
        "\n",
        "        # Check output sentence\n",
        "        for word in output_sentence.split(' '):\n",
        "            if word not in voc.word2index:\n",
        "                keep_output = False\n",
        "                break\n",
        "\n",
        "        # Only keep pairs that do not contain trimmed word(s) in their input or output sentence\n",
        "        if keep_input and keep_output:\n",
        "            keep_pairs.append(pair)\n",
        "\n",
        "    print(\"Trimmed from {} pairs to {}, {:.4f} of total\".format(len(pairs), len(keep_pairs), len(keep_pairs) / len(pairs)))\n",
        "    return keep_pairs\n",
        "\n",
        "# Trim rare words from the vocabulary and filter pairs\n",
        "pairs = trimRareWords(voc, pairs, MIN_COUNT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rK0xOgc_zi_T"
      },
      "source": [
        "These functions prepare the input and output tensors for training data, including padding and creating binary masks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "FebgmPKzzi_U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee2ae0c4-2885-4713-83bb-fc7220cb4a4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_variable: tensor([[ 3796,    20,    63,  1519,    11],\n",
            "        [  122,   256,  2674,  1324,  5834],\n",
            "        [    8,   285,    39,   116,  1297],\n",
            "        [  114,   462,  5183,     6,    14],\n",
            "        [  386,  9671,   181,  1324,     2],\n",
            "        [   55,   997,    59,   116,     0],\n",
            "        [   14, 13658,   685,     6,     0],\n",
            "        [ 3201,    10,    10,     2,     0],\n",
            "        [   78,     2,     2,     0,     0],\n",
            "        [ 8272,     0,     0,     0,     0],\n",
            "        [   14,     0,     0,     0,     0],\n",
            "        [  386,     0,     0,     0,     0],\n",
            "        [  186,     0,     0,     0,     0],\n",
            "        [ 4371,     0,     0,     0,     0],\n",
            "        [   78,     0,     0,     0,     0],\n",
            "        [10810,     0,     0,     0,     0],\n",
            "        [ 7389,     0,     0,     0,     0],\n",
            "        [   14,     0,     0,     0,     0],\n",
            "        [    2,     0,     0,     0,     0]])\n",
            "lengths: tensor([19,  9,  9,  8,  5])\n",
            "target_variable: tensor([[  679, 13657,    19,   128,    34],\n",
            "        [  139,   171,    14,    42,    90],\n",
            "        [  925,    14,     2,    34,    20],\n",
            "        [   14,   549,     0,    20,   860],\n",
            "        [ 3687,    14,     0,   199,    10],\n",
            "        [ 1691, 11876,     0,   106,     2],\n",
            "        [  181,   450,     0,    10,     0],\n",
            "        [  839,  4991,     0,   128,     0],\n",
            "        [   14,    14,     0,   116,     0],\n",
            "        [   14,   194,     0,    10,     0],\n",
            "        [   14,    68,     0,     2,     0],\n",
            "        [  186,  1890,     0,     0,     0],\n",
            "        [   14,    14,     0,     0,     0],\n",
            "        [   14,     2,     0,     0,     0],\n",
            "        [   14,     0,     0,     0,     0],\n",
            "        [   55,     0,     0,     0,     0],\n",
            "        [   14,     0,     0,     0,     0],\n",
            "        [   14,     0,     0,     0,     0],\n",
            "        [   14,     0,     0,     0,     0],\n",
            "        [    2,     0,     0,     0,     0]])\n",
            "mask: tensor([[ True,  True,  True,  True,  True],\n",
            "        [ True,  True,  True,  True,  True],\n",
            "        [ True,  True,  True,  True,  True],\n",
            "        [ True,  True, False,  True,  True],\n",
            "        [ True,  True, False,  True,  True],\n",
            "        [ True,  True, False,  True,  True],\n",
            "        [ True,  True, False,  True, False],\n",
            "        [ True,  True, False,  True, False],\n",
            "        [ True,  True, False,  True, False],\n",
            "        [ True,  True, False,  True, False],\n",
            "        [ True,  True, False,  True, False],\n",
            "        [ True,  True, False, False, False],\n",
            "        [ True,  True, False, False, False],\n",
            "        [ True,  True, False, False, False],\n",
            "        [ True, False, False, False, False],\n",
            "        [ True, False, False, False, False],\n",
            "        [ True, False, False, False, False],\n",
            "        [ True, False, False, False, False],\n",
            "        [ True, False, False, False, False],\n",
            "        [ True, False, False, False, False]])\n",
            "max_target_len: 20\n"
          ]
        }
      ],
      "source": [
        "# Function to convert a sentence to a list of indexes using the vocabulary\n",
        "def indexesFromSentence(voc, sentence):\n",
        "    return [voc.word2index[word] for word in sentence.split(' ')] + [EOS_token]\n",
        "\n",
        "# Function to pad sequences with zeros to create a rectangular matrix\n",
        "def zeroPadding(l, fillvalue=PAD_token):\n",
        "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n",
        "\n",
        "# Function to create a binary matrix indicating the presence of padding tokens\n",
        "def binaryMatrix(l, value=PAD_token):\n",
        "    m = []\n",
        "    for i, seq in enumerate(l):\n",
        "        m.append([])\n",
        "        for token in seq:\n",
        "            if token == PAD_token:\n",
        "                m[i].append(0)\n",
        "            else:\n",
        "                m[i].append(1)\n",
        "    return m\n",
        "\n",
        "# Function to prepare input sequence tensor and lengths\n",
        "def inputVar(l, voc):\n",
        "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
        "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
        "    padList = zeroPadding(indexes_batch)\n",
        "    padVar = torch.LongTensor(padList)\n",
        "    return padVar, lengths\n",
        "\n",
        "# Function to prepare target sequence tensor, padding mask, and max target length\n",
        "def outputVar(l, voc):\n",
        "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
        "    max_target_len = max([len(indexes) for indexes in indexes_batch])\n",
        "    padList = zeroPadding(indexes_batch)\n",
        "    mask = binaryMatrix(padList)\n",
        "    mask = torch.BoolTensor(mask)\n",
        "    padVar = torch.LongTensor(padList)\n",
        "    return padVar, mask, max_target_len\n",
        "\n",
        "# Function to prepare all items for a given batch of pairs\n",
        "def batch2TrainData(voc, pair_batch):\n",
        "    # Sort the pair batch by the length of the input sequences (for efficient padding)\n",
        "    pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
        "    input_batch, output_batch = [], []\n",
        "    for pair in pair_batch:\n",
        "        input_batch.append(pair[0])\n",
        "        output_batch.append(pair[1])\n",
        "    inp, lengths = inputVar(input_batch, voc)\n",
        "    output, mask, max_target_len = outputVar(output_batch, voc)\n",
        "    return inp, lengths, output, mask, max_target_len\n",
        "\n",
        "# Example code for validation\n",
        "small_batch_size = 5\n",
        "# Create batches of training data with random pairs\n",
        "batches = batch2TrainData(voc, [random.choice(pairs) for _ in range(small_batch_size)])\n",
        "input_variable, lengths, target_variable, mask, max_target_len = batches\n",
        "\n",
        "# Print the prepared data for validation\n",
        "print(\"input_variable:\", input_variable)\n",
        "print(\"lengths:\", lengths)\n",
        "print(\"target_variable:\", target_variable)\n",
        "print(\"mask:\", mask)\n",
        "print(\"max_target_len:\", max_target_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsZYHHa0zi_U"
      },
      "source": [
        "### Encoder ###\n",
        "This EncoderRNN class is a part of a sequence-to-sequence (seq2seq) model. It takes input sequences, processes them through a bidirectional GRU (Gated Recurrent Unit), and returns the outputs and the final hidden state:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "7iQcjXVazi_W"
      },
      "outputs": [],
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = embedding\n",
        "\n",
        "        # Initialize GRU; the input_size and hidden_size parameters are both set to 'hidden_size'\n",
        "        #   because our input size is a word embedding with number of features == hidden_size\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,\n",
        "                          dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\n",
        "\n",
        "    def forward(self, input_seq, input_lengths, hidden=None):\n",
        "        # Convert word indexes to embeddings\n",
        "        embedded = self.embedding(input_seq)\n",
        "        # Pack padded batch of sequences for RNN module\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
        "        # Forward pass through GRU\n",
        "        outputs, hidden = self.gru(packed, hidden)\n",
        "        # Unpack padding\n",
        "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
        "        # Sum bidirectional GRU outputs\n",
        "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n",
        "        # Return output and final hidden state\n",
        "        return outputs, hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9oq1g1mzi_W"
      },
      "source": [
        "### Decoder\n",
        "\n",
        "This Attn class is used for implementing different attention mechanisms (dot, general, concat) in a sequence-to-sequence (seq2seq) model. It calculates attention scores based on the chosen method and returns softmax-normalized probability scores:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "deLHyzPxzi_W"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Attn(nn.Module):\n",
        "    def __init__(self, method, hidden_size):\n",
        "        super(Attn, self).__init__()\n",
        "        self.method = method\n",
        "        if self.method not in ['dot', 'general', 'concat']:\n",
        "            raise ValueError(self.method, \"is not an appropriate attention method.\")\n",
        "        self.hidden_size = hidden_size\n",
        "        if self.method == 'general':\n",
        "            # Linear layer for general attention\n",
        "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
        "        elif self.method == 'concat':\n",
        "            # Linear layers for concat attention\n",
        "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
        "            self.v = nn.Parameter(torch.FloatTensor(hidden_size))\n",
        "\n",
        "    def dot_score(self, hidden, encoder_output):\n",
        "        # Calculate dot product attention scores\n",
        "        return torch.sum(hidden * encoder_output, dim=2)\n",
        "\n",
        "    def general_score(self, hidden, encoder_output):\n",
        "        # Calculate general attention scores\n",
        "        energy = self.attn(encoder_output)\n",
        "        return torch.sum(hidden * energy, dim=2)\n",
        "\n",
        "    def concat_score(self, hidden, encoder_output):\n",
        "        # Calculate concat attention scores\n",
        "        energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()\n",
        "        return torch.sum(self.v * energy, dim=2)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        # Calculate the attention weights (energies) based on the given method\n",
        "        if self.method == 'general':\n",
        "            attn_energies = self.general_score(hidden, encoder_outputs)\n",
        "        elif self.method == 'concat':\n",
        "            attn_energies = self.concat_score(hidden, encoder_outputs)\n",
        "        elif self.method == 'dot':\n",
        "            attn_energies = self.dot_score(hidden, encoder_outputs)\n",
        "\n",
        "        # Transpose max_length and batch_size dimensions\n",
        "        attn_energies = attn_energies.t()\n",
        "\n",
        "        # Return the softmax normalized probability scores (with added dimension)\n",
        "        return F.softmax(attn_energies, dim=1).unsqueeze(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ac2n9OnMzi_X"
      },
      "source": [
        "This 'LuongAttnDecoderRNN' class is a part of a sequence-to-sequence (seq2seq) model with Luong attention. It takes an input step (word), processes it through a GRU, calculates attention scores, and produces an output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "gHzWG8MKzi_X"
      },
      "outputs": [],
      "source": [
        "class LuongAttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
        "        super(LuongAttnDecoderRNN, self).__init__()\n",
        "\n",
        "        # Keep for reference\n",
        "        self.attn_model = attn_model\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # Define layers\n",
        "        self.embedding = embedding\n",
        "        self.embedding_dropout = nn.Dropout(dropout)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n",
        "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "        # Initialize attention mechanism\n",
        "        self.attn = Attn(attn_model, hidden_size)\n",
        "\n",
        "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
        "        # Note: we run this one step (word) at a time\n",
        "        # Get embedding of the current input word\n",
        "        embedded = self.embedding(input_step)\n",
        "        embedded = self.embedding_dropout(embedded)\n",
        "        # Forward through unidirectional GRU\n",
        "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
        "        # Calculate attention weights from the current GRU output\n",
        "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
        "        # Multiply attention weights with encoder outputs to get the new \"weighted sum\" context vector\n",
        "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
        "        # Squeeze dimensions to match shapes for concatenation\n",
        "        rnn_output = rnn_output.squeeze(0)\n",
        "        context = context.squeeze(1)\n",
        "        # Concatenate weighted context vector and GRU output using Luong Eq. 5\n",
        "        concat_input = torch.cat((rnn_output, context), 1)\n",
        "        concat_output = torch.tanh(self.concat(concat_input))\n",
        "        # Predict the next word using Luong Eq. 6\n",
        "        output = self.out(concat_output)\n",
        "        output = F.softmax(output, dim=1)\n",
        "        # Return the output and the final hidden state\n",
        "        return output, hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqm-zsJozi_X"
      },
      "source": [
        "## Define Training Procedure\n",
        "\n",
        "### Masked loss\n",
        "\n",
        "This function is used to calculate the negative log-likelihood loss for a sequence of inputs and target indices while handling padding elements using a binary mask.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "rLhSP278zi_X"
      },
      "outputs": [],
      "source": [
        "def maskNLLLoss(inp, target, mask):\n",
        "    # Calculate the negative log-likelihood loss for a sequence of inputs ('inp') and target indices ('target')\n",
        "    # masked by a binary mask ('mask') to handle padding.\n",
        "\n",
        "    # Calculate the total number of elements (tokens) in the batch\n",
        "    nTotal = mask.sum()\n",
        "\n",
        "    # Use the 'gather' function to select the predicted probabilities corresponding to the target indices\n",
        "    # and calculate the cross-entropy loss\n",
        "    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n",
        "\n",
        "    # Mask the loss using the provided binary mask to ignore padding elements\n",
        "    loss = crossEntropy.masked_select(mask).mean()\n",
        "\n",
        "    # Ensure the loss is on the same device as the input\n",
        "    loss = loss.to(device)\n",
        "\n",
        "    # Return the masked loss and the total number of non-padding elements in the batch\n",
        "    return loss, nTotal.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4g0zrs-zi_X"
      },
      "source": [
        "### Single training iteration\n",
        "\n",
        "This train function is used to train a sequence-to-sequence model with teacher forcing. It performs a forward pass through the encoder and decoder, calculates the loss, and performs backpropagation to update the model weights:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "gal7ubGHzi_X"
      },
      "outputs": [],
      "source": [
        "def train(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding,\n",
        "          encoder_optimizer, decoder_optimizer, batch_size, clip, max_length=MAX_LENGTH):\n",
        "\n",
        "    # Zero gradients\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    # Set device options\n",
        "    input_variable = input_variable.to(device)\n",
        "    target_variable = target_variable.to(device)\n",
        "    mask = mask.to(device)\n",
        "    # Lengths for RNN packing should always be on the CPU\n",
        "    lengths = lengths.to(\"cpu\")\n",
        "\n",
        "    # Initialise variables\n",
        "    loss = 0\n",
        "    print_losses = []\n",
        "    n_totals = 0\n",
        "\n",
        "    # Forward pass through encoder\n",
        "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
        "\n",
        "    # Create initial decoder input (start with SOS tokens for each sentence)\n",
        "    decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])\n",
        "    decoder_input = decoder_input.to(device)\n",
        "\n",
        "    # Set initial decoder hidden state to the encoder's final hidden state\n",
        "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
        "\n",
        "    # Determine if we are using teacher forcing this iteration\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    # Forward batch of sequences through decoder one time step at a time\n",
        "    if use_teacher_forcing:\n",
        "        for t in range(max_target_len):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs\n",
        "            )\n",
        "            # Teacher forcing: next input is current target\n",
        "            decoder_input = target_variable[t].view(1, -1)\n",
        "            # Calculate and accumulate loss\n",
        "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
        "            loss += mask_loss\n",
        "            print_losses.append(mask_loss.item() * nTotal)\n",
        "            n_totals += nTotal\n",
        "    else:\n",
        "        for t in range(max_target_len):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs\n",
        "            )\n",
        "            # No teacher forcing: next input is decoder's own current output\n",
        "            _, topi = decoder_output.topk(1)\n",
        "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
        "            decoder_input = decoder_input.to(device)\n",
        "            # Calculate and accumulate loss\n",
        "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
        "            loss += mask_loss\n",
        "            print_losses.append(mask_loss.item() * nTotal)\n",
        "            n_totals += nTotal\n",
        "\n",
        "    # Perform backpropagation\n",
        "    loss.backward()\n",
        "\n",
        "    # Clip gradients: gradients are modified in place\n",
        "    _ = nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
        "    _ = nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
        "\n",
        "    # Adjust model weights\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return sum(print_losses) / n_totals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BpUnJgCzi_Y"
      },
      "source": [
        "### Training iterations\n",
        "\n",
        "This trainIters function is used to train a sequence-to-sequence model for a specified number of iterations. It loads batches of training data, runs training iterations, and saves checkpoints at specified intervals:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "6d1kpfuHzi_Y"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import torch\n",
        "import random  # Make sure to import random if you're using it\n",
        "\n",
        "def trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
        "               embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size,\n",
        "               print_every, save_every, clip, corpus_name, loadFilename):\n",
        "\n",
        "    training_batches = [batch2TrainData(voc, [random.choice(pairs) for _ in range(batch_size)])\n",
        "                        for _ in range(n_iteration)]\n",
        "\n",
        "    print('Initialising ...')\n",
        "    start_iteration = 1\n",
        "    print_loss = 0\n",
        "    if loadFilename:\n",
        "        start_iteration = checkpoint['iteration'] + 1\n",
        "\n",
        "    # Prepare to store loss values for plotting\n",
        "    loss_values = []\n",
        "\n",
        "    print(\"Training...\")\n",
        "    for iteration in range(start_iteration, n_iteration + 1):\n",
        "        training_batch = training_batches[iteration - 1]\n",
        "        input_variable, lengths, target_variable, mask, max_target_len = training_batch\n",
        "\n",
        "        loss = train(input_variable, lengths, target_variable, mask, max_target_len, encoder,\n",
        "                     decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip)\n",
        "        print_loss += loss\n",
        "\n",
        "        # Print progress and append to loss_values every print_every iterations\n",
        "        if iteration % print_every == 0:\n",
        "            print_loss_avg = print_loss / print_every\n",
        "            print(\"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\".format(\n",
        "                iteration, iteration / n_iteration * 100, print_loss_avg))\n",
        "            print_loss = 0\n",
        "            loss_values.append(print_loss_avg)\n",
        "\n",
        "        # Save checkpoint every save_every iterations\n",
        "        if iteration % save_every == 0:\n",
        "            directory = os.path.join(save_dir, model_name, corpus_name, '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, 'hidden_size_placeholder'))  # Replace 'hidden_size_placeholder' with actual value\n",
        "            if not os.path.exists(directory):\n",
        "                os.makedirs(directory)\n",
        "            torch.save({\n",
        "                'iteration': iteration,\n",
        "                'en': encoder.state_dict(),\n",
        "                'de': decoder.state_dict(),\n",
        "                'en_opt': encoder_optimizer.state_dict(),\n",
        "                'de_opt': decoder_optimizer.state_dict(),\n",
        "                'loss': loss,\n",
        "                'voc_dict': voc.__dict__,\n",
        "                'embedding': embedding.state_dict()\n",
        "            }, os.path.join(directory, '{}_{}.tar'.format(iteration, 'checkpoint')))\n",
        "\n",
        "    # Plot the loss over time after the training loop\n",
        "    plt.figure()\n",
        "    x_values = range(print_every, print_every * len(loss_values) + 1, print_every)\n",
        "    plt.plot(x_values, loss_values, marker='o')\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel('Average Loss')\n",
        "    plt.title('Training Loss Over Time')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # Return the loss values for further analysis or use\n",
        "    return loss_values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-dpwGMWzi_Y"
      },
      "source": [
        "## Define Evaluation\n",
        "\n",
        "### Greedy decoding\n",
        "This GreedySearchDecoder class is used for performing greedy decoding with a trained encoder-decoder model. It takes an input sequence, feeds it through the encoder, and then iteratively generates the most likely output sequence one token at a time using the decoder:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "shckFdbXzi_Y"
      },
      "outputs": [],
      "source": [
        "class GreedySearchDecoder(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(GreedySearchDecoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, input_seq, input_length, max_length):\n",
        "        # Forward input through encoder model\n",
        "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
        "        # Prepare encoder's final hidden layer to be the first hidden input to the decoder\n",
        "        decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
        "        # Initialize decoder input with SOS_token\n",
        "        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n",
        "        # Initialize tensors to append decoded words to\n",
        "        all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n",
        "        all_scores = torch.zeros([0], device=device)\n",
        "        # Iteratively decode one word token at a time\n",
        "        for _ in range(max_length):\n",
        "            # Forward pass through decoder\n",
        "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            # Obtain the most likely word token and its softmax score\n",
        "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
        "            # Record token and score\n",
        "            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
        "            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
        "            # Prepare the current token to be the next decoder input (add a dimension)\n",
        "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
        "        # Return collections of word tokens and scores\n",
        "        return all_tokens, all_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NijbRg0vzi_Y"
      },
      "source": [
        "### Evaluate my text\n",
        "\n",
        "These functions are used for evaluating input sentences using a trained encoder-decoder model and a searcher (e.g., GreedySearchDecoder). The evaluate function takes an input sentence, processes it, and generates a response based on the model's predictions. The evaluateInput function allows users to interact with the model by providing input sentences and receiving responses:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "RZVqt17dzi_Y"
      },
      "outputs": [],
      "source": [
        "def evaluate(encoder, decoder, searcher, voc, sentence, max_length=MAX_LENGTH):\n",
        "    ### Format input sentence as a batch\n",
        "    # words -> indexes\n",
        "    indexes_batch = [indexesFromSentence(voc, sentence)]\n",
        "    # Create lengths tensor\n",
        "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
        "    # Transpose dimensions of the batch to match models' expectations\n",
        "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
        "    # Use the appropriate device\n",
        "    input_batch = input_batch.to(device)\n",
        "    lengths = lengths.to(\"cpu\")\n",
        "    # Decode sentence with searcher\n",
        "    tokens, scores = searcher(input_batch, lengths, max_length)\n",
        "    # indexes -> words\n",
        "    decoded_words = [voc.index2word[token.item()] for token in tokens]\n",
        "    return decoded_words\n",
        "\n",
        "\n",
        "def evaluateInput(encoder, decoder, searcher, voc):\n",
        "    input_sentence = ''\n",
        "    while(1):\n",
        "        try:\n",
        "            # Get input sentence\n",
        "            input_sentence = input('> ')\n",
        "            # Check if it is a quit case\n",
        "            if input_sentence == 'q' or input_sentence == 'quit':\n",
        "                break\n",
        "            # Normalize sentence\n",
        "            input_sentence = normalizeString(input_sentence)\n",
        "            # Evaluate sentence\n",
        "            output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
        "            # Format and print the response sentence\n",
        "            output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
        "            print('Bot:', ' '.join(output_words))\n",
        "\n",
        "        except KeyError:\n",
        "            print(\"Error: Encountered an unknown word.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcIqg9XNzi_Z"
      },
      "source": [
        "## Run Model\n",
        "\n",
        "These configurations define various aspects of your model, such as its name, attention mechanism, hidden size, number of layers, dropout rate, batch size, and whether you want to load a pretrained model checkpoint:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "yYBTBixGzi_Z"
      },
      "outputs": [],
      "source": [
        "# Configure models\n",
        "model_name = 'cb_model'  # Choose a name for your model\n",
        "#attn_model = 'dot'       # Choose an attention mechanism: 'dot', 'general', or 'concat'\n",
        "attn_model = 'general'  # Uncomment this line to use the 'general' attention mechanism\n",
        "#attn_model = 'concat'   # Uncomment this line to use the 'concat' attention mechanism\n",
        "hidden_size = 500        # Define the hidden size for the encoder and decoder\n",
        "encoder_n_layers = 2     # Number of layers for the encoder\n",
        "decoder_n_layers = 2     # Number of layers for the decoder\n",
        "dropout = 0.01            # Dropout probability for the model\n",
        "batch_size = 128         # Batch size for training\n",
        "\n",
        "# Set checkpoint to load from; set to None if starting from scratch\n",
        "loadFilename = None      # Set to a file path if you want to load a pretrained model checkpoint\n",
        "checkpoint_iter = 4000  # Specify the checkpoint iteration to load (if loadFilename is not None)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_CXr0D2zi_Z"
      },
      "source": [
        "Sample code to load from a checkpoint:\n",
        "\n",
        "```python\n",
        "loadFilename = os.path.join(save_dir, model_name, corpus_name,\n",
        "                    '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size),\n",
        "                    '{}_checkpoint.tar'.format(checkpoint_iter))\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "OZb7AyNnzi_e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b710894e-417c-495a-d4a1-5e2354b39ee1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Models built and ready to go!\n"
          ]
        }
      ],
      "source": [
        "# Load model if a `loadFilename` is provided\n",
        "if loadFilename:\n",
        "    # If loading on the same machine the model was trained on\n",
        "    checkpoint = torch.load(loadFilename)\n",
        "    # If loading a model trained on GPU to CPU, use the following line instead\n",
        "    # checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n",
        "\n",
        "    # Load model weights and other relevant data\n",
        "    encoder_sd = checkpoint['en']                    # Encoder state dictionary\n",
        "    decoder_sd = checkpoint['de']                    # Decoder state dictionary\n",
        "    encoder_optimizer_sd = checkpoint['en_opt']      # Encoder optimizer state dictionary\n",
        "    decoder_optimizer_sd = checkpoint['de_opt']      # Decoder optimizer state dictionary\n",
        "    embedding_sd = checkpoint['embedding']           # Embedding state dictionary\n",
        "    voc.__dict__ = checkpoint['voc_dict']             # Vocabulary dictionary\n",
        "\n",
        "# Initialize word embeddings\n",
        "embedding = nn.Embedding(voc.num_words, hidden_size)\n",
        "if loadFilename:\n",
        "    embedding.load_state_dict(embedding_sd)\n",
        "\n",
        "# Initialize encoder & decoder models\n",
        "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
        "decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout)\n",
        "\n",
        "if loadFilename:\n",
        "    # Load the pretrained model weights\n",
        "    encoder.load_state_dict(encoder_sd)\n",
        "    decoder.load_state_dict(decoder_sd)\n",
        "\n",
        "# Move models to the appropriate device (GPU or CPU)\n",
        "encoder = encoder.to(device)\n",
        "decoder = decoder.to(device)\n",
        "\n",
        "print('Models built and ready to go!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCkK_dtpzi_e"
      },
      "source": [
        "### Run Training ###\n",
        "\n",
        "This code block sets up various training parameters, including gradient clipping, teacher forcing ratio, learning rates, and the number of training iterations. It also initializes the optimizer for both the encoder and decoder. Finally, it starts the training loop by calling the trainIters function with the provided parameters.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "vs4LAMrszi_f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ab688a87-f384-49d1-93af-17953e4d86f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building optimisers ...\n",
            "Starting Training!\n",
            "Initialising ...\n",
            "Training...\n",
            "Iteration: 100; Percent complete: 1.0%; Average loss: 5.7959\n",
            "Iteration: 200; Percent complete: 2.0%; Average loss: 5.1588\n",
            "Iteration: 300; Percent complete: 3.0%; Average loss: 4.9476\n",
            "Iteration: 400; Percent complete: 4.0%; Average loss: 4.7917\n",
            "Iteration: 500; Percent complete: 5.0%; Average loss: 4.7937\n",
            "Iteration: 600; Percent complete: 6.0%; Average loss: 4.6163\n",
            "Iteration: 700; Percent complete: 7.0%; Average loss: 4.5173\n",
            "Iteration: 800; Percent complete: 8.0%; Average loss: 4.5252\n",
            "Iteration: 900; Percent complete: 9.0%; Average loss: 4.5650\n",
            "Iteration: 1000; Percent complete: 10.0%; Average loss: 4.4603\n",
            "Iteration: 1100; Percent complete: 11.0%; Average loss: 4.4489\n",
            "Iteration: 1200; Percent complete: 12.0%; Average loss: 4.4551\n",
            "Iteration: 1300; Percent complete: 13.0%; Average loss: 4.3807\n",
            "Iteration: 1400; Percent complete: 14.0%; Average loss: 4.2438\n",
            "Iteration: 1500; Percent complete: 15.0%; Average loss: 4.4423\n",
            "Iteration: 1600; Percent complete: 16.0%; Average loss: 4.4616\n",
            "Iteration: 1700; Percent complete: 17.0%; Average loss: 4.3206\n",
            "Iteration: 1800; Percent complete: 18.0%; Average loss: 4.2926\n",
            "Iteration: 1900; Percent complete: 19.0%; Average loss: 4.3291\n",
            "Iteration: 2000; Percent complete: 20.0%; Average loss: 4.3889\n",
            "Iteration: 2100; Percent complete: 21.0%; Average loss: 4.3019\n",
            "Iteration: 2200; Percent complete: 22.0%; Average loss: 4.1593\n",
            "Iteration: 2300; Percent complete: 23.0%; Average loss: 4.2732\n",
            "Iteration: 2400; Percent complete: 24.0%; Average loss: 4.1882\n",
            "Iteration: 2500; Percent complete: 25.0%; Average loss: 4.2836\n",
            "Iteration: 2600; Percent complete: 26.0%; Average loss: 4.2929\n",
            "Iteration: 2700; Percent complete: 27.0%; Average loss: 4.1833\n",
            "Iteration: 2800; Percent complete: 28.0%; Average loss: 4.1730\n",
            "Iteration: 2900; Percent complete: 29.0%; Average loss: 4.0898\n",
            "Iteration: 3000; Percent complete: 30.0%; Average loss: 4.1482\n",
            "Iteration: 3100; Percent complete: 31.0%; Average loss: 4.2352\n",
            "Iteration: 3200; Percent complete: 32.0%; Average loss: 4.1480\n",
            "Iteration: 3300; Percent complete: 33.0%; Average loss: 4.2653\n",
            "Iteration: 3400; Percent complete: 34.0%; Average loss: 4.0870\n",
            "Iteration: 3500; Percent complete: 35.0%; Average loss: 4.1104\n",
            "Iteration: 3600; Percent complete: 36.0%; Average loss: 4.1954\n",
            "Iteration: 3700; Percent complete: 37.0%; Average loss: 4.1730\n",
            "Iteration: 3800; Percent complete: 38.0%; Average loss: 4.0601\n",
            "Iteration: 3900; Percent complete: 39.0%; Average loss: 4.0408\n",
            "Iteration: 4000; Percent complete: 40.0%; Average loss: 4.0301\n",
            "Iteration: 4100; Percent complete: 41.0%; Average loss: 4.1180\n",
            "Iteration: 4200; Percent complete: 42.0%; Average loss: 4.0328\n",
            "Iteration: 4300; Percent complete: 43.0%; Average loss: 4.0933\n",
            "Iteration: 4400; Percent complete: 44.0%; Average loss: 3.9659\n",
            "Iteration: 4500; Percent complete: 45.0%; Average loss: 4.0246\n",
            "Iteration: 4600; Percent complete: 46.0%; Average loss: 4.0761\n",
            "Iteration: 4700; Percent complete: 47.0%; Average loss: 4.0077\n",
            "Iteration: 4800; Percent complete: 48.0%; Average loss: 3.9570\n",
            "Iteration: 4900; Percent complete: 49.0%; Average loss: 3.9481\n",
            "Iteration: 5000; Percent complete: 50.0%; Average loss: 4.0215\n",
            "Iteration: 5100; Percent complete: 51.0%; Average loss: 4.0325\n",
            "Iteration: 5200; Percent complete: 52.0%; Average loss: 3.9007\n",
            "Iteration: 5300; Percent complete: 53.0%; Average loss: 4.0379\n",
            "Iteration: 5400; Percent complete: 54.0%; Average loss: 3.9645\n",
            "Iteration: 5500; Percent complete: 55.0%; Average loss: 4.0826\n",
            "Iteration: 5600; Percent complete: 56.0%; Average loss: 3.9069\n",
            "Iteration: 5700; Percent complete: 57.0%; Average loss: 3.9302\n",
            "Iteration: 5800; Percent complete: 58.0%; Average loss: 3.9600\n",
            "Iteration: 5900; Percent complete: 59.0%; Average loss: 3.9365\n",
            "Iteration: 6000; Percent complete: 60.0%; Average loss: 3.9665\n",
            "Iteration: 6100; Percent complete: 61.0%; Average loss: 4.0310\n",
            "Iteration: 6200; Percent complete: 62.0%; Average loss: 4.0553\n",
            "Iteration: 6300; Percent complete: 63.0%; Average loss: 3.7495\n",
            "Iteration: 6400; Percent complete: 64.0%; Average loss: 3.6980\n",
            "Iteration: 6500; Percent complete: 65.0%; Average loss: 3.7856\n",
            "Iteration: 6600; Percent complete: 66.0%; Average loss: 3.7338\n",
            "Iteration: 6700; Percent complete: 67.0%; Average loss: 3.8884\n",
            "Iteration: 6800; Percent complete: 68.0%; Average loss: 3.8241\n",
            "Iteration: 6900; Percent complete: 69.0%; Average loss: 3.9214\n",
            "Iteration: 7000; Percent complete: 70.0%; Average loss: 3.7583\n",
            "Iteration: 7100; Percent complete: 71.0%; Average loss: 3.8162\n",
            "Iteration: 7200; Percent complete: 72.0%; Average loss: 3.6716\n",
            "Iteration: 7300; Percent complete: 73.0%; Average loss: 3.6382\n",
            "Iteration: 7400; Percent complete: 74.0%; Average loss: 3.7415\n",
            "Iteration: 7500; Percent complete: 75.0%; Average loss: 3.7736\n",
            "Iteration: 7600; Percent complete: 76.0%; Average loss: 3.7774\n",
            "Iteration: 7700; Percent complete: 77.0%; Average loss: 3.5392\n",
            "Iteration: 7800; Percent complete: 78.0%; Average loss: 3.6223\n",
            "Iteration: 7900; Percent complete: 79.0%; Average loss: 3.7388\n",
            "Iteration: 8000; Percent complete: 80.0%; Average loss: 3.6994\n",
            "Iteration: 8100; Percent complete: 81.0%; Average loss: 3.7441\n",
            "Iteration: 8200; Percent complete: 82.0%; Average loss: 3.7402\n",
            "Iteration: 8300; Percent complete: 83.0%; Average loss: 3.6916\n",
            "Iteration: 8400; Percent complete: 84.0%; Average loss: 3.7176\n",
            "Iteration: 8500; Percent complete: 85.0%; Average loss: 3.6240\n",
            "Iteration: 8600; Percent complete: 86.0%; Average loss: 3.5362\n",
            "Iteration: 8700; Percent complete: 87.0%; Average loss: 3.5832\n",
            "Iteration: 8800; Percent complete: 88.0%; Average loss: 3.6903\n",
            "Iteration: 8900; Percent complete: 89.0%; Average loss: 3.6365\n",
            "Iteration: 9000; Percent complete: 90.0%; Average loss: 3.5937\n",
            "Iteration: 9100; Percent complete: 91.0%; Average loss: 3.5304\n",
            "Iteration: 9200; Percent complete: 92.0%; Average loss: 3.6934\n",
            "Iteration: 9300; Percent complete: 93.0%; Average loss: 3.4230\n",
            "Iteration: 9400; Percent complete: 94.0%; Average loss: 3.6381\n",
            "Iteration: 9500; Percent complete: 95.0%; Average loss: 3.6285\n",
            "Iteration: 9600; Percent complete: 96.0%; Average loss: 3.5541\n",
            "Iteration: 9700; Percent complete: 97.0%; Average loss: 3.4841\n",
            "Iteration: 9800; Percent complete: 98.0%; Average loss: 3.4664\n",
            "Iteration: 9900; Percent complete: 99.0%; Average loss: 3.6619\n",
            "Iteration: 10000; Percent complete: 100.0%; Average loss: 3.4517\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[5.79585596440193,\n",
              " 5.158836472667,\n",
              " 4.94755381444455,\n",
              " 4.791713713891906,\n",
              " 4.793701493325575,\n",
              " 4.616315167015185,\n",
              " 4.517290163506621,\n",
              " 4.525191675594025,\n",
              " 4.5650161945682575,\n",
              " 4.460287178907377,\n",
              " 4.4488937252992296,\n",
              " 4.45509426048469,\n",
              " 4.380748904908698,\n",
              " 4.243750515249484,\n",
              " 4.44231362312365,\n",
              " 4.461577130029006,\n",
              " 4.320558533454686,\n",
              " 4.2926383999455435,\n",
              " 4.3290882991789275,\n",
              " 4.388923506660292,\n",
              " 4.30187153565468,\n",
              " 4.159268759589429,\n",
              " 4.273160785663854,\n",
              " 4.1881692997879725,\n",
              " 4.283638836126308,\n",
              " 4.292947153849367,\n",
              " 4.183258055530579,\n",
              " 4.173034660901456,\n",
              " 4.0897893203775775,\n",
              " 4.148231960175232,\n",
              " 4.235187940177993,\n",
              " 4.148049565009848,\n",
              " 4.265323619138607,\n",
              " 4.087028618452515,\n",
              " 4.1104289693651115,\n",
              " 4.195415301431458,\n",
              " 4.173040515254308,\n",
              " 4.060116193582008,\n",
              " 4.040796931902688,\n",
              " 4.030087230712285,\n",
              " 4.1179671681262064,\n",
              " 4.032848461228182,\n",
              " 4.093277613342854,\n",
              " 3.965907998370546,\n",
              " 4.024620065759967,\n",
              " 4.076060476433547,\n",
              " 4.007711158549615,\n",
              " 3.9569904232584427,\n",
              " 3.948092744083805,\n",
              " 4.021544818263415,\n",
              " 4.032479669258132,\n",
              " 3.9007230276756677,\n",
              " 4.037869450954634,\n",
              " 3.9644613864858376,\n",
              " 4.082574483418378,\n",
              " 3.9068747334653473,\n",
              " 3.930224487380944,\n",
              " 3.9599569075320424,\n",
              " 3.936451341074646,\n",
              " 3.9664953557296774,\n",
              " 4.031006405651748,\n",
              " 4.055348756095157,\n",
              " 3.749489253304115,\n",
              " 3.6980058650403778,\n",
              " 3.785562510794457,\n",
              " 3.7338490186487134,\n",
              " 3.8883877013904833,\n",
              " 3.8240659159645833,\n",
              " 3.9213927405466587,\n",
              " 3.7582724633090043,\n",
              " 3.8162357300233656,\n",
              " 3.671640645314204,\n",
              " 3.6381997775915753,\n",
              " 3.741453997764042,\n",
              " 3.7735894187435424,\n",
              " 3.7773599081066527,\n",
              " 3.5392464939973114,\n",
              " 3.6223145599067936,\n",
              " 3.7387664052318064,\n",
              " 3.6993720836317356,\n",
              " 3.744096932208671,\n",
              " 3.7401969327938023,\n",
              " 3.691556382716657,\n",
              " 3.717577883908415,\n",
              " 3.624010816619578,\n",
              " 3.5362280827990555,\n",
              " 3.58321871528744,\n",
              " 3.690259153734349,\n",
              " 3.6364894962782732,\n",
              " 3.5936970157543446,\n",
              " 3.530446258173135,\n",
              " 3.6933941100457797,\n",
              " 3.422960765385661,\n",
              " 3.6380675867519283,\n",
              " 3.6285171612028364,\n",
              " 3.5540908272998633,\n",
              " 3.4840931701553735,\n",
              " 3.466443837611302,\n",
              " 3.661855787709316,\n",
              " 3.4516915676651347]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "# Configure training/optimisation\n",
        "clip = 50.0                   # Gradient clipping threshold to prevent exploding gradients\n",
        "teacher_forcing_ratio = 0.7   # Probability of using teacher forcing during training (1.0 means always)\n",
        "learning_rate = 0.0001        # Learning rate for the optimiser\n",
        "decoder_learning_ratio = 5.0  # Learning rate multiplier for the decoder compared to the encoder\n",
        "n_iteration = 10000            # Number of training iterations\n",
        "print_every = 100           # Print training progress every `print_every` iterations\n",
        "save_every = 500             # Save a checkpoint every `save_every` iterations\n",
        "\n",
        "# Ensure dropout layers are in train mode\n",
        "encoder.train()\n",
        "decoder.train()\n",
        "\n",
        "# Initialise optimisers\n",
        "print('Building optimisers ...')\n",
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
        "\n",
        "# Load optimiser states if a checkpoint is provided\n",
        "if loadFilename:\n",
        "    encoder_optimizer.load_state_dict(encoder_optimizer_sd)\n",
        "    decoder_optimizer.load_state_dict(decoder_optimizer_sd)\n",
        "\n",
        "# If you have CUDA, configure CUDA to call\n",
        "for state in encoder_optimizer.state.values():\n",
        "    for k, v in state.items():\n",
        "        if isinstance(v, torch.Tensor):\n",
        "            state[k] = v.cuda()\n",
        "\n",
        "for state in decoder_optimizer.state.values():\n",
        "    for k, v in state.items():\n",
        "        if isinstance(v, torch.Tensor):\n",
        "            state[k] = v.cuda()\n",
        "\n",
        "# Run training iterations\n",
        "print(\"Starting Training!\")\n",
        "trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
        "           embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size,\n",
        "           print_every, save_every, clip, corpus_name, loadFilename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9bVadTTzi_f"
      },
      "source": [
        "### Run Evaluation\n",
        "\n",
        "To chat with your model, run the following block.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "f_N5hle9zi_f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "443fa6a0-d1bb-4cdb-d1cd-33178958a136"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> Hey\n",
            "Bot: hey . . . . . .\n",
            "> How are you?\n",
            "Bot: i m fine . .\n",
            "> Who am I?\n",
            "Bot: you re not . . . . . . .\n",
            "> Not who?\n",
            "Bot: you . . .\n",
            "> I feel this way a lot.\n",
            "Bot: i m not . . . . .\n",
            "> Where are you?\n",
            "Bot: i m in the bathroom . . .\n",
            "> Who are you??\n",
            "Bot: i m . . . . . . .\n",
            "> ...?\n",
            "Bot: . . . he s banging . . . . . . . . .\n",
            "> What?!?\n",
            "Bot: i don t . . . . . . .\n",
            "> q\n"
          ]
        }
      ],
      "source": [
        "# Set dropout layers to ``eval`` mode\n",
        "encoder.eval()\n",
        "decoder.eval()\n",
        "\n",
        "# Initialise search module\n",
        "searcher = GreedySearchDecoder(encoder, decoder)\n",
        "\n",
        "# Begin chatting (uncomment and run the following line to begin)\n",
        "evaluateInput(encoder, decoder, searcher, voc)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}